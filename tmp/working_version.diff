# diff --git a/configs/e2e_mask_rcnn_R_50_C4_1x.yaml b/configs/e2e_mask_rcnn_R_50_C4_1x.yaml
# index 27373e1..bfcd258 100644
# --- a/configs/e2e_mask_rcnn_R_50_C4_1x.yaml
# +++ b/configs/e2e_mask_rcnn_R_50_C4_1x.yaml
# @@ -7,12 +7,10 @@ MODEL:
#    ROI_MASK_HEAD:
#      PREDICTOR: "MaskRCNNC4Predictor"
#      SHARE_BOX_FEATURE_EXTRACTOR: True
# -  ROI_BOX_HEAD:
# -    POOLER_SAMPLING_RATIO: 2
#    MASK_ON: True
#  DATASETS:
# -  TRAIN: ("coco_2017_train",)
# -  TEST: ("coco_2017_val",)
# +  TRAIN: ("coco_2014_train", "coco_2014_valminusminival")
# +  TEST: ("coco_2014_minival",)
#  SOLVER:
#    BASE_LR: 0.01
#    WEIGHT_DECAY: 0.0001
diff --git a/configs/e2e_mask_rcnn_R_50_FPN_1x.yaml b/configs/e2e_mask_rcnn_R_50_FPN_1x.yaml
index edab8a5..7f5a894 100644
--- a/configs/e2e_mask_rcnn_R_50_FPN_1x.yaml
+++ b/configs/e2e_mask_rcnn_R_50_FPN_1x.yaml
@@ -30,8 +30,8 @@ MODEL:
     SHARE_BOX_FEATURE_EXTRACTOR: False
   MASK_ON: True
 DATASETS:
-  TRAIN: ("coco_2017_train", )
-  TEST: ("coco_2017_val",)
+  TRAIN: ("coco_2014_train", "coco_2014_valminusminival")
+  TEST: ("coco_2014_minival",)
 DATALOADER:
   SIZE_DIVISIBILITY: 32
 SOLVER:
# diff --git a/demo/Mask_R-CNN_demo.py b/demo/Mask_R-CNN_demo.py
# deleted file mode 100644
# index f4c5ff4..0000000
# --- a/demo/Mask_R-CNN_demo.py
# +++ /dev/null
# @@ -1,139 +0,0 @@
# -#!/usr/bin/env python
# -# coding: utf-8
# -
# -# # Mask R-CNN demo
# -#
# -# This notebook illustrates one possible way of using `maskrcnn_benchmark` for computing predictions on images from an arbitrary URL.
# -#
# -# Let's start with a few standard imports
# -
# -# In[1]:
# -
# -
# -import matplotlib.pyplot as plt
# -import matplotlib.pylab as pylab
# -
# -import requests
# -from io import BytesIO
# -from PIL import Image
# -import numpy as np
# -
# -
# -# In[2]:
# -
# -
# -# this makes our figures bigger
# -pylab.rcParams['figure.figsize'] = 20, 12
# -
# -
# -# Those are the relevant imports for the detection model
# -
# -# In[3]:
# -
# -
# -from maskrcnn_benchmark.config import cfg
# -from predictor import COCODemo
# -
# -
# -# We provide a helper class `COCODemo`, which loads a model from the config file, and performs pre-processing, model prediction and post-processing for us.
# -#
# -# We can configure several model options by overriding the config options.
# -# In here, we make the model run on the CPU
# -
# -# In[4]:
# -
# -
# -config_file = "../configs/caffe2/e2e_mask_rcnn_R_50_FPN_1x_caffe2.yaml"
# -
# -# update the config options with the config file
# -cfg.merge_from_file(config_file)
# -# manual override some options
# -cfg.merge_from_list(["MODEL.DEVICE", "cpu"])
# -
# -
# -# Now we create the `COCODemo` object. It contains a few extra options for conveniency, such as the confidence threshold for detections to be shown.
# -
# -# In[5]:
# -
# -
# -coco_demo = COCODemo(
# -    cfg,
# -    min_image_size=800,
# -    confidence_threshold=0.7,
# -)
# -
# -
# -# Let's define a few helper functions for loading images from a URL
# -
# -# In[6]:
# -
# -
# -def load(url):
# -    """
# -    Given an url of an image, downloads the image and
# -    returns a PIL image
# -    """
# -    response = requests.get(url)
# -    pil_image = Image.open(BytesIO(response.content)).convert("RGB")
# -    # convert to BGR format
# -    image = np.array(pil_image)[:, :, [2, 1, 0]]
# -    return image
# -
# -def imshow(img):
# -    plt.imshow(img[:, :, [2, 1, 0]])
# -    plt.axis("off")
# -
# -def imsave(img):
# -    plt.imsave('predicted.png', img[:, :, [2, 1, 0]])
# -
# -
# -# Let's now load an image from the COCO dataset. It's reference is in the comment
# -
# -# In[7]:
# -
# -
# -# from http://cocodataset.org/#explore?id=345434
# -image = load("http://farm3.staticflickr.com/2469/3915380994_2e611b1779_z.jpg")
# -# imshow(image)
# -
# -
# -# ### Computing the predictions
# -#
# -# We provide a `run_on_opencv_image` function, which takes an image as it was loaded by OpenCV (in `BGR` format), and computes the predictions on them, returning an image with the predictions overlayed on the image.
# -
# -# In[8]:
# -
# -
# -# compute predictions
# -predictions = coco_demo.run_on_opencv_image(image)
# -import pdb
# -pdb.set_trace()
# -# imshow(predictions)
# -imsave(predictions)
# -
# -# # ## Keypoints Demo
# -
# -# # In[9]:
# -
# -
# -# # set up demo for keypoints
# -# config_file = "../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml"
# -# cfg.merge_from_file(config_file)
# -# cfg.merge_from_list(["MODEL.DEVICE", "cpu"])
# -# cfg.merge_from_list(["MODEL.MASK_ON", False])
# -
# -# coco_demo = COCODemo(
# -    # cfg,
# -    # min_image_size=800,
# -    # confidence_threshold=0.7,
# -# )
# -
# -
# -# # In[10]:
# -
# -
# -# # run demo
# -# image = load("http://farm9.staticflickr.com/8419/8710147224_ff637cc4fc_z.jpg")
# -# predictions = coco_demo.run_on_opencv_image(image)
# -# imshow(predictions)
-
# diff --git a/demo/maskrcnn_xla_demo.py b/demo/maskrcnn_xla_demo.py
# deleted file mode 100644
# index 4acdb1f..0000000
# --- a/demo/maskrcnn_xla_demo.py
# +++ /dev/null
# @@ -1,80 +0,0 @@
# -
# -import matplotlib.pyplot as plt
# -import matplotlib.pylab as pylab
# -
# -import requests
# -from io import BytesIO
# -from PIL import Image
# -import numpy as np
# -from maskrcnn_benchmark.config import cfg
# -from predictor import COCODemo
# -import torch_xla
# -import torch_xla_py.utils as xu
# -import torch_xla_py.xla_model as xm
# -
# -
# -pylab.rcParams['figure.figsize'] = 20, 12
# -
# -
# -# We provide a helper class `COCODemo`, which loads a model from the config file, and performs pre-processing, model prediction and post-processing for us.
# -#
# -# We can configure several model options by overriding the config options.
# -# In here, we make the model run on the CPU
# -
# -config_file = "../configs/caffe2/e2e_mask_rcnn_R_50_FPN_1x_caffe2.yaml"
# -
# -# update the config options with the config file
# -cfg.merge_from_file(config_file)
# -# manual override some options
# -# cfg.merge_from_list(["MODEL.DEVICE", "cpu"])
# -
# -
# -# Let's define a few helper functions for loading images from a URL
# -def load(url):
# -    """
# -    Given an url of an image, downloads the image and
# -    returns a PIL image
# -    """
# -    response = requests.get(url)
# -    pil_image = Image.open(BytesIO(response.content)).convert("RGB")
# -    # convert to BGR format
# -    image = np.array(pil_image)[:, :, [2, 1, 0]]
# -    return image
# -
# -
# -def imshow(img):
# -    plt.imshow(img[:, :, [2, 1, 0]])
# -    plt.axis("off")
# -
# -
# -def imsave(img):
# -    plt.imsave('predicted.png', img[:, :, [2, 1, 0]])
# -
# -
# -# Let's now load an image from the COCO dataset. It's reference is in the comment
# -# from http://cocodataset.org/#explore?id=345434
# -image = load("http://farm3.staticflickr.com/2469/3915380994_2e611b1779_z.jpg")
# -# imshow(image)
# -
# -# ### Computing the predictions
# -#
# -# We provide a `run_on_opencv_image` function, which takes an image as it was loaded by OpenCV (in `BGR` format), and computes the predictions on them, returning an image with the predictions overlayed on the image.
# -
# -# compute predictions
# -xla_device = xm.xla_device()
# -# Now we create the `COCODemo` object. It contains a few extra options for conveniency, such as the confidence threshold for detections to be shown.
# -
# -coco_demo = COCODemo(
# -    cfg,
# -    min_image_size=800,
# -    confidence_threshold=0.7,
# -    device=xla_device,
# -)
# -
# -# DEBUG with a small input
# -# shape = image.shape
# -# image = image[0:shape[0]//20, 0:shape[1]//20, :]
# -predictions = coco_demo.run_on_opencv_image(image)
# -print(torch_xla._XLAC._xla_metrics_report())
# -
# -imsave(predictions)
# diff --git a/demo/predicted.png b/demo/predicted.png
# deleted file mode 100644
# index bbdb90d..0000000
# Binary files a/demo/predicted.png and /dev/null differ
# diff --git a/demo/predictor.py b/demo/predictor.py
# index 4acb346..b152fda 100644
# --- a/demo/predictor.py
# +++ b/demo/predictor.py
# @@ -104,15 +104,11 @@ class COCODemo(object):
#          show_mask_heatmaps=False,
#          masks_per_dim=2,
#          min_image_size=224,
# -        device=None,
#      ):
#          self.cfg = cfg.clone()
#          self.model = build_detection_model(cfg)
#          self.model.eval()
# -        if device is not None:
# -            self.device = device
# -        else:
# -            self.device = torch.device(cfg.MODEL.DEVICE)
# +        self.device = torch.device(cfg.MODEL.DEVICE)
#          self.model.to(self.device)
#          self.min_image_size = min_image_size

# @@ -199,7 +195,7 @@ class COCODemo(object):
#                  the BoxList via `prediction.fields()`
#          """
#          # apply pre-processing to image
# -        image = self.transforms(original_image).to(self.device)
# +        image = self.transforms(original_image)
#          # convert to an ImageList, padded so that it is divisible by
#          # cfg.DATALOADER.SIZE_DIVISIBILITY
#          image_list = to_image_list(image, self.cfg.DATALOADER.SIZE_DIVISIBILITY)
# diff --git a/demo/repro.py b/demo/repro.py
# deleted file mode 100644
# index 32ebabb..0000000
# --- a/demo/repro.py
# +++ /dev/null
# @@ -1,13 +0,0 @@
# -import torch
# -import torch_xla
# -import torch_xla_py.utils as xu
# -import torch_xla_py.xla_model as xm
# -import pdb
# -xla_device = xm.xla_device()
# -xla_device = 'cpu'
# -d = torch.rand(3, 1, 2).to(xla_device)
# -a = torch.rand(4, 2).to(xla_device)
# -b = torch.max(a, d)
# -pdb.set_trace()
# -print(b)
# -
# diff --git a/maskrcnn_benchmark/csrc/ROIAlign.h b/maskrcnn_benchmark/csrc/ROIAlign.h
# index fbc254f..3907dea 100644
# --- a/maskrcnn_benchmark/csrc/ROIAlign.h
# +++ b/maskrcnn_benchmark/csrc/ROIAlign.h
# @@ -1,3 +1,4 @@
# +// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
#  #pragma once

#  #include "cpu/vision.h"
# @@ -7,70 +8,39 @@
#  #endif

#  // Interface for Python
# -at::Tensor ROIAlign_forward(
# -    const at::Tensor& input, // Input feature map.
# -    const at::Tensor& rois, // List of ROIs to pool over.
# -    const float spatial_scale, // The scale of the image features. ROIs will be
# -    // scaled to this.
# -    const int pooled_height, // The height of the pooled feature map.
# -    const int pooled_width, // The width of the pooled feature
# -    const int sampling_ratio) // The number of points to sample in each bin
# -// along each axis.
# -{
# +at::Tensor ROIAlign_forward(const at::Tensor& input,
# +                            const at::Tensor& rois,
# +                            const float spatial_scale,
# +                            const int pooled_height,
# +                            const int pooled_width,
# +                            const int sampling_ratio) {
#    if (input.type().is_cuda()) {
#  #ifdef WITH_CUDA
# -    return ROIAlign_forward_cuda(
# -        input,
# -        rois,
# -        spatial_scale,
# -        pooled_height,
# -        pooled_width,
# -        sampling_ratio);
# +    return ROIAlign_forward_cuda(input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio);
#  #else
#      AT_ERROR("Not compiled with GPU support");
#  #endif
#    }
# -  return ROIAlign_forward_cpu(
# -      input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio);
# +  return ROIAlign_forward_cpu(input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio);
#  }

# -at::Tensor ROIAlign_backward(
# -    const at::Tensor& grad,
# -    const at::Tensor& rois,
# -    const float spatial_scale,
# -    const int pooled_height,
# -    const int pooled_width,
# -    const int batch_size,
# -    const int channels,
# -    const int height,
# -    const int width,
# -    const int sampling_ratio) {
# +at::Tensor ROIAlign_backward(const at::Tensor& grad,
# +                             const at::Tensor& rois,
# +                             const float spatial_scale,
# +                             const int pooled_height,
# +                             const int pooled_width,
# +                             const int batch_size,
# +                             const int channels,
# +                             const int height,
# +                             const int width,
# +                             const int sampling_ratio) {
#    if (grad.type().is_cuda()) {
#  #ifdef WITH_CUDA
# -    return ROIAlign_backward_cuda(
# -        grad,
# -        rois,
# -        spatial_scale,
# -        pooled_height,
# -        pooled_width,
# -        batch_size,
# -        channels,
# -        height,
# -        width,
# -        sampling_ratio);
# +    return ROIAlign_backward_cuda(grad, rois, spatial_scale, pooled_height, pooled_width, batch_size, channels, height, width, sampling_ratio);
#  #else
#      AT_ERROR("Not compiled with GPU support");
#  #endif
#    }
# -  return ROIAlign_backward_cpu(
# -      grad.cpu(),
# -      rois.cpu(),
# -      spatial_scale,
# -      pooled_height,
# -      pooled_width,
# -      batch_size,
# -      channels,
# -      height,
# -      width,
# -      sampling_ratio);
# +  AT_ERROR("Not implemented on the CPU");
#  }
+
# diff --git a/maskrcnn_benchmark/csrc/cpu/ROIAlign_cpu.cpp b/maskrcnn_benchmark/csrc/cpu/ROIAlign_cpu.cpp
# index f854455..d35aedf 100644
# --- a/maskrcnn_benchmark/csrc/cpu/ROIAlign_cpu.cpp
# +++ b/maskrcnn_benchmark/csrc/cpu/ROIAlign_cpu.cpp
# @@ -1,4 +1,4 @@
# -#include <ATen/TensorUtils.h>
# +// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
#  #include "cpu/vision.h"

#  // implementation taken from Caffe2
# @@ -111,9 +111,9 @@ void pre_calc_for_bilinear_interpolate(
#  }

#  template <typename T>
# -void ROIAlignForward(
# +void ROIAlignForward_cpu_kernel(
#      const int nthreads,
# -    const T* input,
# +    const T* bottom_data,
#      const T& spatial_scale,
#      const int channels,
#      const int height,
# @@ -121,8 +121,12 @@ void ROIAlignForward(
#      const int pooled_height,
#      const int pooled_width,
#      const int sampling_ratio,
# -    const T* rois,
# -    T* output) {
# +    const T* bottom_rois,
# +    //int roi_cols,
# +    T* top_data) {
# +  //AT_ASSERT(roi_cols == 4 || roi_cols == 5);
# +  int roi_cols = 5;
# +
#    int n_rois = nthreads / channels / pooled_width / pooled_height;
#    // (n, c, ph, pw) is an element in the pooled output
#    // can be parallelized using omp
# @@ -130,18 +134,23 @@ void ROIAlignForward(
#    for (int n = 0; n < n_rois; n++) {
#      int index_n = n * channels * pooled_width * pooled_height;

# -    const T* offset_rois = rois + n * 5;
# -    int roi_batch_ind = offset_rois[0];
# +    // roi could have 4 or 5 columns
# +    const T* offset_bottom_rois = bottom_rois + n * roi_cols;
# +    int roi_batch_ind = 0;
# +    if (roi_cols == 5) {
# +      roi_batch_ind = offset_bottom_rois[0];
# +      offset_bottom_rois++;
# +    }

#      // Do not using rounding; this implementation detail is critical
# -    T roi_start_w = offset_rois[1] * spatial_scale;
# -    T roi_start_h = offset_rois[2] * spatial_scale;
# -    T roi_end_w = offset_rois[3] * spatial_scale;
# -    T roi_end_h = offset_rois[4] * spatial_scale;
# -    // T roi_start_w = round(offset_rois[0] * spatial_scale);
# -    // T roi_start_h = round(offset_rois[1] * spatial_scale);
# -    // T roi_end_w = round(offset_rois[2] * spatial_scale);
# -    // T roi_end_h = round(offset_rois[3] * spatial_scale);
# +    T roi_start_w = offset_bottom_rois[0] * spatial_scale;
# +    T roi_start_h = offset_bottom_rois[1] * spatial_scale;
# +    T roi_end_w = offset_bottom_rois[2] * spatial_scale;
# +    T roi_end_h = offset_bottom_rois[3] * spatial_scale;
# +    // T roi_start_w = round(offset_bottom_rois[0] * spatial_scale);
# +    // T roi_start_h = round(offset_bottom_rois[1] * spatial_scale);
# +    // T roi_end_w = round(offset_bottom_rois[2] * spatial_scale);
# +    // T roi_end_h = round(offset_bottom_rois[3] * spatial_scale);

#      // Force malformed ROIs to be 1x1
#      T roi_width = std::max(roi_end_w - roi_start_w, (T)1.);
# @@ -178,10 +187,10 @@ void ROIAlignForward(
#          roi_bin_grid_w,
#          pre_calc);

# -    for (int c = 0; c < channels; c++) {
# +      for (int c = 0; c < channels; c++) {
#        int index_n_c = index_n + c * pooled_width * pooled_height;
# -      const T* offset_input =
# -          input + (roi_batch_ind * channels + c) * height * width;
# +      const T* offset_bottom_data =
# +          bottom_data + (roi_batch_ind * channels + c) * height * width;
#        int pre_calc_index = 0;

#        for (int ph = 0; ph < pooled_height; ph++) {
# @@ -192,284 +201,57 @@ void ROIAlignForward(
#            for (int iy = 0; iy < roi_bin_grid_h; iy++) {
#              for (int ix = 0; ix < roi_bin_grid_w; ix++) {
#                PreCalc<T> pc = pre_calc[pre_calc_index];
# -              output_val += pc.w1 * offset_input[pc.pos1] +
# -                  pc.w2 * offset_input[pc.pos2] +
# -                  pc.w3 * offset_input[pc.pos3] + pc.w4 * offset_input[pc.pos4];
# +              output_val += pc.w1 * offset_bottom_data[pc.pos1] +
# +                  pc.w2 * offset_bottom_data[pc.pos2] +
# +                  pc.w3 * offset_bottom_data[pc.pos3] +
# +                  pc.w4 * offset_bottom_data[pc.pos4];

#                pre_calc_index += 1;
#              }
#            }
#            output_val /= count;

# -          output[index] = output_val;
# +          top_data[index] = output_val;
#          } // for pw
#        } // for ph
#      } // for c
#    } // for n
#  }

# -template <typename T>
# -void bilinear_interpolate_gradient(
# -    const int height,
# -    const int width,
# -    T y,
# -    T x,
# -    T& w1,
# -    T& w2,
# -    T& w3,
# -    T& w4,
# -    int& x_low,
# -    int& x_high,
# -    int& y_low,
# -    int& y_high,
# -    const int index /* index for debug only*/) {
# -  // deal with cases that inverse elements are out of feature map boundary
# -  if (y < -1.0 || y > height || x < -1.0 || x > width) {
# -    // empty
# -    w1 = w2 = w3 = w4 = 0.;
# -    x_low = x_high = y_low = y_high = -1;
# -    return;
# -  }
# -
# -  if (y <= 0)
# -    y = 0;
# -  if (x <= 0)
# -    x = 0;
# -
# -  y_low = (int)y;
# -  x_low = (int)x;
# -
# -  if (y_low >= height - 1) {
# -    y_high = y_low = height - 1;
# -    y = (T)y_low;
# -  } else {
# -    y_high = y_low + 1;
# -  }
# -
# -  if (x_low >= width - 1) {
# -    x_high = x_low = width - 1;
# -    x = (T)x_low;
# -  } else {
# -    x_high = x_low + 1;
# -  }
# -
# -  T ly = y - y_low;
# -  T lx = x - x_low;
# -  T hy = 1. - ly, hx = 1. - lx;
# -
# -  // reference in forward
# -  // T v1 = input[y_low * width + x_low];
# -  // T v2 = input[y_low * width + x_high];
# -  // T v3 = input[y_high * width + x_low];
# -  // T v4 = input[y_high * width + x_high];
# -  // T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);
# -
# -  w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;
# -
# -  return;
# -}
# -
# -template <class T>
# -inline void add(T* address, const T& val) {
# -  *address += val;
# -}
# -
# -template <typename T>
# -void ROIAlignBackward(
# -    const int nthreads,
# -    const T* grad_output,
# -    const T& spatial_scale,
# -    const int channels,
# -    const int height,
# -    const int width,
# -    const int pooled_height,
# -    const int pooled_width,
# -    const int sampling_ratio,
# -    T* grad_input,
# -    const T* rois,
# -    const int n_stride,
# -    const int c_stride,
# -    const int h_stride,
# -    const int w_stride) {
# -  for (int index = 0; index < nthreads; index++) {
# -    // (n, c, ph, pw) is an element in the pooled output
# -    int pw = index % pooled_width;
# -    int ph = (index / pooled_width) % pooled_height;
# -    int c = (index / pooled_width / pooled_height) % channels;
# -    int n = index / pooled_width / pooled_height / channels;
# -
# -    const T* offset_rois = rois + n * 5;
# -    int roi_batch_ind = offset_rois[0];
# -
# -    // Do not using rounding; this implementation detail is critical
# -    T roi_start_w = offset_rois[1] * spatial_scale;
# -    T roi_start_h = offset_rois[2] * spatial_scale;
# -    T roi_end_w = offset_rois[3] * spatial_scale;
# -    T roi_end_h = offset_rois[4] * spatial_scale;
# -
# -    // Force malformed ROIs to be 1x1
# -    T roi_width = std::max(roi_end_w - roi_start_w, (T)1.);
# -    T roi_height = std::max(roi_end_h - roi_start_h, (T)1.);
# -    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);
# -    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);
# -
# -    T* offset_grad_input =
# -        grad_input + ((roi_batch_ind * channels + c) * height * width);
# -
# -    int output_offset = n * n_stride + c * c_stride;
# -    const T* offset_grad_output = grad_output + output_offset;
# -    const T grad_output_this_bin =
# -        offset_grad_output[ph * h_stride + pw * w_stride];
# -
# -    // We use roi_bin_grid to sample the grid and mimic integral
# -    int roi_bin_grid_h = (sampling_ratio > 0)
# -        ? sampling_ratio
# -        : ceil(roi_height / pooled_height); // e.g., = 2
# -    int roi_bin_grid_w =
# -        (sampling_ratio > 0) ? sampling_ratio : ceil(roi_width / pooled_width);
# -
# -    // We do average (integral) pooling inside a bin
# -    const T count = roi_bin_grid_h * roi_bin_grid_w; // e.g. = 4
# -
# -    for (int iy = 0; iy < roi_bin_grid_h; iy++) {
# -      const T y = roi_start_h + ph * bin_size_h +
# -          static_cast<T>(iy + .5f) * bin_size_h /
# -              static_cast<T>(roi_bin_grid_h); // e.g., 0.5, 1.5
# -      for (int ix = 0; ix < roi_bin_grid_w; ix++) {
# -        const T x = roi_start_w + pw * bin_size_w +
# -            static_cast<T>(ix + .5f) * bin_size_w /
# -                static_cast<T>(roi_bin_grid_w);
# -
# -        T w1, w2, w3, w4;
# -        int x_low, x_high, y_low, y_high;
# -
# -        bilinear_interpolate_gradient(
# -            height,
# -            width,
# -            y,
# -            x,
# -            w1,
# -            w2,
# -            w3,
# -            w4,
# -            x_low,
# -            x_high,
# -            y_low,
# -            y_high,
# -            index);
# -
# -        T g1 = grad_output_this_bin * w1 / count;
# -        T g2 = grad_output_this_bin * w2 / count;
# -        T g3 = grad_output_this_bin * w3 / count;
# -        T g4 = grad_output_this_bin * w4 / count;
# -
# -        if (x_low >= 0 && x_high >= 0 && y_low >= 0 && y_high >= 0) {
# -          // atomic add is not needed for now since it is single threaded
# -          add(offset_grad_input + y_low * width + x_low, static_cast<T>(g1));
# -          add(offset_grad_input + y_low * width + x_high, static_cast<T>(g2));
# -          add(offset_grad_input + y_high * width + x_low, static_cast<T>(g3));
# -          add(offset_grad_input + y_high * width + x_high, static_cast<T>(g4));
# -        } // if
# -      } // ix
# -    } // iy
# -  } // for
# -} // ROIAlignBackward
# -
# -at::Tensor ROIAlign_forward_cpu(
# -    const at::Tensor& input,
# -    const at::Tensor& rois,
# -    const float spatial_scale,
# -    const int pooled_height,
# -    const int pooled_width,
# -    const int sampling_ratio) {
# -  AT_ASSERTM(input.device().is_cpu(), "input must be a CPU tensor");
# -  AT_ASSERTM(rois.device().is_cpu(), "rois must be a CPU tensor");
# -
# -  at::TensorArg input_t{input, "input", 1}, rois_t{rois, "rois", 2};
# -
# -  at::CheckedFrom c = "ROIAlign_forward_cpu";
# -  at::checkAllSameType(c, {input_t, rois_t});
# +at::Tensor ROIAlign_forward_cpu(const at::Tensor& input,
# +                                const at::Tensor& rois,
# +                                const float spatial_scale,
# +                                const int pooled_height,
# +                                const int pooled_width,
# +                                const int sampling_ratio) {
# +  AT_ASSERTM(!input.type().is_cuda(), "input must be a CPU tensor");
# +  AT_ASSERTM(!rois.type().is_cuda(), "rois must be a CPU tensor");

#    auto num_rois = rois.size(0);
#    auto channels = input.size(1);
#    auto height = input.size(2);
#    auto width = input.size(3);

# -  at::Tensor output = at::zeros(
# -      {num_rois, channels, pooled_height, pooled_width}, input.options());
# -
# +  auto output = at::empty({num_rois, channels, pooled_height, pooled_width}, input.options());
#    auto output_size = num_rois * pooled_height * pooled_width * channels;

# -  if (output.numel() == 0)
# +  if (output.numel() == 0) {
#      return output;
# -
# -  AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), "ROIAlign_forward", [&] {
# -    ROIAlignForward<scalar_t>(
# -        output_size,
# -        input.contiguous().data<scalar_t>(),
# -        spatial_scale,
# -        channels,
# -        height,
# -        width,
# -        pooled_height,
# -        pooled_width,
# -        sampling_ratio,
# -        rois.contiguous().data<scalar_t>(),
# -        output.data<scalar_t>());
# -  });
# -  return output;
# -}
# -
# -at::Tensor ROIAlign_backward_cpu(
# -    const at::Tensor& grad,
# -    const at::Tensor& rois,
# -    const float spatial_scale,
# -    const int pooled_height,
# -    const int pooled_width,
# -    const int batch_size,
# -    const int channels,
# -    const int height,
# -    const int width,
# -    const int sampling_ratio) {
# -  AT_ASSERTM(grad.device().is_cpu(), "grad must be a CPU tensor");
# -  AT_ASSERTM(rois.device().is_cpu(), "rois must be a CPU tensor");
# -
# -  at::TensorArg grad_t{grad, "grad", 1}, rois_t{rois, "rois", 2};
# -
# -  at::CheckedFrom c = "ROIAlign_backward_cpu";
# -  at::checkAllSameType(c, {grad_t, rois_t});
# -
# -  at::Tensor grad_input =
# -      at::zeros({batch_size, channels, height, width}, grad.options());
# -
# -  // handle possibly empty gradients
# -  if (grad.numel() == 0) {
# -    return grad_input;
#    }

# -  // get stride values to ensure indexing into gradients is correct.
# -  int n_stride = grad.stride(0);
# -  int c_stride = grad.stride(1);
# -  int h_stride = grad.stride(2);
# -  int w_stride = grad.stride(3);
# -
# -  AT_DISPATCH_FLOATING_TYPES_AND_HALF(grad.type(), "ROIAlign_forward", [&] {
# -    ROIAlignBackward<scalar_t>(
# -        grad.numel(),
# -        grad.contiguous().data<scalar_t>(),
# -        spatial_scale,
# -        channels,
# -        height,
# -        width,
# -        pooled_height,
# -        pooled_width,
# -        sampling_ratio,
# -        grad_input.data<scalar_t>(),
# -        rois.contiguous().data<scalar_t>(),
# -        n_stride,
# -        c_stride,
# -        h_stride,
# -        w_stride);
# +  AT_DISPATCH_FLOATING_TYPES(input.type(), "ROIAlign_forward", [&] {
# +    ROIAlignForward_cpu_kernel<scalar_t>(
# +         output_size,
# +         input.data<scalar_t>(),
# +         spatial_scale,
# +         channels,
# +         height,
# +         width,
# +         pooled_height,
# +         pooled_width,
# +         sampling_ratio,
# +         rois.data<scalar_t>(),
# +         output.data<scalar_t>());
#    });
# -  return grad_input;
# +  return output;
#  }
# diff --git a/maskrcnn_benchmark/csrc/cpu/nms_cpu.cpp b/maskrcnn_benchmark/csrc/cpu/nms_cpu.cpp
# index 55d2ed9..1153dea 100644
# --- a/maskrcnn_benchmark/csrc/cpu/nms_cpu.cpp
# +++ b/maskrcnn_benchmark/csrc/cpu/nms_cpu.cpp
# @@ -6,14 +6,13 @@ template <typename scalar_t>
#  at::Tensor nms_cpu_kernel(const at::Tensor& dets,
#                            const at::Tensor& scores,
#                            const float threshold) {
# -  //AT_ASSERTM(!dets.type().is_cuda(), "dets must be a CPU tensor");
# -  //AT_ASSERTM(!scores.type().is_cuda(), "scores must be a CPU tensor");
# -  //AT_ASSERTM(dets.type() == scores.type(), "dets should have the same type as scores");
# +  AT_ASSERTM(!dets.type().is_cuda(), "dets must be a CPU tensor");
# +  AT_ASSERTM(!scores.type().is_cuda(), "scores must be a CPU tensor");
# +  AT_ASSERTM(dets.type() == scores.type(), "dets should have the same type as scores");

#    if (dets.numel() == 0) {
#      return at::empty({0}, dets.options().dtype(at::kLong).device(at::kCPU));
#    }
# -  std::cout << "hey" << std::endl;

#    auto x1_t = dets.select(1, 0).contiguous();
#    auto y1_t = dets.select(1, 1).contiguous();
# diff --git a/maskrcnn_benchmark/csrc/cpu/vision.h b/maskrcnn_benchmark/csrc/cpu/vision.h
# index 9dbc748..9261125 100644
# --- a/maskrcnn_benchmark/csrc/cpu/vision.h
# +++ b/maskrcnn_benchmark/csrc/cpu/vision.h
# @@ -2,25 +2,13 @@
#  #pragma once
#  #include <torch/extension.h>

# -at::Tensor ROIAlign_forward_cpu(
# -    const at::Tensor& input,
# -    const at::Tensor& rois,
# -    const float spatial_scale,
# -    const int pooled_height,
# -    const int pooled_width,
# -    const int sampling_ratio);

# -at::Tensor ROIAlign_backward_cpu(
# -    const at::Tensor& grad,
# -    const at::Tensor& rois,
# -    const float spatial_scale,
# -    const int pooled_height,
# -    const int pooled_width,
# -    const int batch_size,
# -    const int channels,
# -    const int height,
# -    const int width,
# -    const int sampling_ratio);
# +at::Tensor ROIAlign_forward_cpu(const at::Tensor& input,
# +                                const at::Tensor& rois,
# +                                const float spatial_scale,
# +                                const int pooled_height,
# +                                const int pooled_width,
# +                                const int sampling_ratio);


#  at::Tensor nms_cpu(const at::Tensor& dets,
# diff --git a/maskrcnn_benchmark/engine/trainer.py b/maskrcnn_benchmark/engine/trainer.py
# index 0c41634..38a9e52 100644
# --- a/maskrcnn_benchmark/engine/trainer.py
# +++ b/maskrcnn_benchmark/engine/trainer.py
# @@ -9,8 +9,6 @@ import torch.distributed as dist
#  from maskrcnn_benchmark.utils.comm import get_world_size
#  from maskrcnn_benchmark.utils.metric_logger import MetricLogger

# -import torch_xla
# -import torch_xla_py.xla_model as xm

#  def reduce_loss_dict(loss_dict):
#      """
# @@ -68,7 +66,6 @@ def do_train(
#          loss_dict = model(images, targets)

#          losses = sum(loss for loss in loss_dict.values())
# -        # print(torch_xla._XLAC._xla_metrics_report())

#          # reduce losses over all GPUs for logging purposes
#          loss_dict_reduced = reduce_loss_dict(loss_dict)
# @@ -77,9 +74,8 @@ def do_train(

#          optimizer.zero_grad()
#          losses.backward()
# -        # optimizer.step()
# -        xm.optimizer_step(optimizer)
# -        print(torch_xla._XLAC._xla_metrics_report())
# +        optimizer.step()
# +
#          batch_time = time.time() - end
#          end = time.time()
#          meters.update(time=batch_time, data=data_time)
# @@ -87,7 +83,7 @@ def do_train(
#          eta_seconds = meters.time.global_avg * (max_iter - iteration)
#          eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))

# -        if iteration % 1 == 0 or iteration == max_iter:
# +        if iteration % 20 == 0 or iteration == max_iter:
#              logger.info(
#                  meters.delimiter.join(
#                      [
# @@ -95,14 +91,14 @@ def do_train(
#                          "iter: {iter}",
#                          "{meters}",
#                          "lr: {lr:.6f}",
# -                        # "max mem: {memory:.0f}",
# +                        "max mem: {memory:.0f}",
#                      ]
#                  ).format(
#                      eta=eta_string,
#                      iter=iteration,
#                      meters=str(meters),
#                      lr=optimizer.param_groups[0]["lr"],
# -                    # memory=torch.cuda.max_memory_allocated() / 1024.0 / 1024.0,
# +                    memory=torch.cuda.max_memory_allocated() / 1024.0 / 1024.0,
#                  )
#              )
#          if iteration % checkpoint_period == 0:
# diff --git a/maskrcnn_benchmark/layers/__init__.py b/maskrcnn_benchmark/layers/__init__.py
# index d201c34..bab50ab 100644
# --- a/maskrcnn_benchmark/layers/__init__.py
# +++ b/maskrcnn_benchmark/layers/__init__.py
# @@ -11,12 +11,11 @@ from .roi_align import ROIAlign
#  from .roi_align import roi_align
#  from .roi_pool import ROIPool
#  from .roi_pool import roi_pool
# -from .smooth_l1_loss import smooth_l1_loss, smooth_l1_loss_no_reduction
# +from .smooth_l1_loss import smooth_l1_loss
#  from .sigmoid_focal_loss import SigmoidFocalLoss
# -from .tensor_roi_align import tensor_roi_align

#  __all__ = ["nms", "roi_align", "ROIAlign", "roi_pool", "ROIPool",
# -           "smooth_l1_loss", "smooth_l1_loss_no_reduction", "Conv2d", "ConvTranspose2d",
# -           "interpolate", "BatchNorm2d", "FrozenBatchNorm2d", "SigmoidFocalLoss"
# +           "smooth_l1_loss", "Conv2d", "ConvTranspose2d", "interpolate",
# +           "BatchNorm2d", "FrozenBatchNorm2d", "SigmoidFocalLoss"
#            ]

# diff --git a/maskrcnn_benchmark/layers/roi_align.py b/maskrcnn_benchmark/layers/roi_align.py
# index 5d71422..170c8f1 100644
# --- a/maskrcnn_benchmark/layers/roi_align.py
# +++ b/maskrcnn_benchmark/layers/roi_align.py
# @@ -6,7 +6,6 @@ from torch.autograd.function import once_differentiable
#  from torch.nn.modules.utils import _pair

#  from maskrcnn_benchmark import _C
# -from maskrcnn_benchmark.layers.tensor_roi_align import tensor_roi_align


#  class _ROIAlign(Function):
# @@ -56,13 +55,6 @@ class ROIAlign(nn.Module):
#          self.sampling_ratio = sampling_ratio

#      def forward(self, input, rois):
# -        if self.sampling_ratio > 0 and input.device.type == 'xla':
# -            batch_size = input.size(0)
# -            num_rois = rois.size(0)
# -            rois = rois[:, 1:].reshape(batch_size, num_rois, -1)
# -            return tensor_roi_align(
# -                input, rois, self.output_size, self.spatial_scale, self.sampling_ratio
# -            )
#          return roi_align(
#              input, rois, self.output_size, self.spatial_scale, self.sampling_ratio
#          )
# diff --git a/maskrcnn_benchmark/layers/smooth_l1_loss.py b/maskrcnn_benchmark/layers/smooth_l1_loss.py
# index 9eee9d6..9c4664b 100644
# --- a/maskrcnn_benchmark/layers/smooth_l1_loss.py
# +++ b/maskrcnn_benchmark/layers/smooth_l1_loss.py
# @@ -4,16 +4,13 @@ import torch

#  # TODO maybe push this to nn?
#  def smooth_l1_loss(input, target, beta=1. / 9, size_average=True):
# -    loss = smooth_l1_loss_no_reduction(input, target, beta, size_average)
# -    if size_average:
# -        return loss.mean()
# -    return loss.sum()
# -
# -def smooth_l1_loss_no_reduction(input, target, beta=1. / 9, size_average=True):
#      """
#      very similar to the smooth_l1_loss from pytorch, but with
#      the extra beta parameter
#      """
#      n = torch.abs(input - target)
#      cond = n < beta
# -    return torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)
# +    loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)
# +    if size_average:
# +        return loss.mean()
# +    return loss.sum()
# diff --git a/maskrcnn_benchmark/layers/tensor_roi_align.py b/maskrcnn_benchmark/layers/tensor_roi_align.py
# deleted file mode 100644
# index 4ecc2c6..0000000
# --- a/maskrcnn_benchmark/layers/tensor_roi_align.py
# +++ /dev/null
# @@ -1,116 +0,0 @@
# -import torch
# -import torch.nn.functional as F
# -
# -
# -def _bottom_data_slice(bottom_data, y, x, height, width):
# -  batch = y.size(0)
# -  num_filters = bottom_data.size(1)
# -  bottom_data = bottom_data.permute([0, 2, 3, 1])
# -  bottom_data = bottom_data.view([-1, num_filters])
# -  output_height = y.size(2)
# -  output_width = x.size(2)
# -  y = y.unsqueeze(3)
# -  x = x.unsqueeze(2)
# -  batch_off = torch.arange(batch) * height * width
# -  batch_off = batch_off.unsqueeze(1).unsqueeze(2).unsqueeze(3)
# -  linear_indices = (y * width + x + batch_off).view([-1])
# -  gathered = torch.index_select(bottom_data, 0, linear_indices)
# -  roi_count = y.size(1)
# -  gathered = gathered.view([batch, roi_count, output_height, output_width, num_filters])
# -  gathered = gathered.permute([0, 1, 4, 2, 3])
# -  return gathered
# -
# -
# -def _bilinear_interpolate(bottom_data, height, width, y, x):
# -  y_low = torch.where(y.long() >= height - 1, torch.full_like(y, height - 1),
# -                      y.long().float()).long()
# -  y_low = y_low.clamp(0, height - 1)
# -  x_low = torch.where(x.long() >= width - 1, torch.full_like(x, width - 1),
# -                      x.long().float()).long()
# -  x_low = x_low.clamp(0, width - 1)
# -
# -  y_high = torch.where(y.long() >= height - 1,
# -                       torch.full_like(y_low, height - 1), y_low + 1)
# -  y_high = y_high.clamp(0, height - 1)
# -  x_high = torch.where(x.long() >= width - 1, torch.full_like(x_low, width - 1),
# -                       x_low + 1)
# -  x_high = x_high.clamp(0, width - 1)
# -
# -  y = torch.where(y.long() >= height - 1, y_low.float(), y)
# -  x = torch.where(x.long() >= width - 1, x_low.float(), x)
# -
# -  ly = (y - y_low.float()).unsqueeze(3)
# -  lx = (x - x_low.float()).unsqueeze(2)
# -  hy = 1. - ly
# -  hx = 1. - lx
# -
# -  v1 = _bottom_data_slice(bottom_data, y_low, x_low, height, width)
# -  v2 = _bottom_data_slice(bottom_data, y_low, x_high, height, width)
# -  v3 = _bottom_data_slice(bottom_data, y_high, x_low, height, width)
# -  v4 = _bottom_data_slice(bottom_data, y_high, x_high, height, width)
# -
# -  w1 = (hy * hx).unsqueeze(2)
# -  w2 = (hy * lx).unsqueeze(2)
# -  w3 = (ly * hx).unsqueeze(2)
# -  w4 = (ly * lx).unsqueeze(2)
# -
# -  val = w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4
# -  y = y.unsqueeze(2).unsqueeze(4).expand_as(val)
# -  x = x.unsqueeze(2).unsqueeze(3).expand_as(val)
# -  val = torch.where(y < -1, torch.zeros_like(val), val)
# -  val = torch.where(y > height, torch.zeros_like(val), val)
# -  val = torch.where(x < -1, torch.zeros_like(val), val)
# -  val = torch.where(x > height, torch.zeros_like(val), val)
# -
# -  return val
# -
# -
# -def tensor_roi_align(bottom_data, bottom_rois, pooled_size, spatial_scale,
# -                     sampling_ratio):
# -  pooled_height = pooled_size[0]
# -  pooled_width = pooled_size[1]
# -
# -  roi_sizes = bottom_rois * spatial_scale
# -  roi_start_w = roi_sizes[:, :, 0]
# -  roi_start_h = roi_sizes[:, :, 1]
# -  roi_end_w = roi_sizes[:, :, 2]
# -  roi_end_h = roi_sizes[:, :, 3]
# -
# -  roi_width = torch.max(roi_end_w - roi_start_w, torch.ones_like(roi_end_w))
# -  roi_height = torch.max(roi_end_h - roi_start_h, torch.ones_like(roi_end_h))
# -  bin_size_h = roi_height / pooled_height
# -  bin_size_w = roi_width / pooled_width
# -
# -  pw = torch.tensor(
# -      [pw for pw in range(pooled_width) for _ in range(sampling_ratio)])
# -  ph = torch.tensor(
# -      [ph for ph in range(pooled_height) for _ in range(sampling_ratio)])
# -  x_neigh_offsets = torch.tensor([pw + .5 for pw in range(sampling_ratio)] *
# -                                 pooled_width)
# -  y_neigh_offsets = torch.tensor([ph + .5 for ph in range(sampling_ratio)] *
# -                                 pooled_height)
# -
# -  ph = ph.unsqueeze(0).unsqueeze(0)
# -  pw = pw.unsqueeze(0).unsqueeze(0)
# -  bin_size_h = bin_size_h.unsqueeze(2)
# -  bin_size_w = bin_size_w.unsqueeze(2)
# -  batch = bottom_data.size(0)
# -  y_neigh_offsets = y_neigh_offsets.unsqueeze(0).unsqueeze(0)
# -  x_neigh_offsets = x_neigh_offsets.unsqueeze(0).unsqueeze(0)
# -
# -  y_neigh_offsets = y_neigh_offsets.expand(batch, 1, y_neigh_offsets.size(2))
# -  x_neigh_offsets = x_neigh_offsets.expand(batch, 1, x_neigh_offsets.size(2))
# -  ph = ph.expand(batch, 1, ph.size(2))
# -  pw = pw.expand(batch, 1, pw.size(2))
# -  roi_start_h = roi_start_h.unsqueeze(2)
# -  roi_start_w = roi_start_w.unsqueeze(2)
# -  y = roi_start_h + bin_size_h * ph.float(
# -  ) + bin_size_h * y_neigh_offsets / sampling_ratio
# -  x = roi_start_w + bin_size_w * pw.float(
# -  ) + bin_size_w * x_neigh_offsets / sampling_ratio
# -
# -  height = bottom_data.size(2)
# -  width = bottom_data.size(3)
# -  interpolated = _bilinear_interpolate(bottom_data, height, width, y, x)
# -  interpolated = interpolated.view(-1, interpolated.size(2), interpolated.size(3), interpolated.size(4))
# -  return F.avg_pool2d(interpolated, sampling_ratio)
# diff --git a/maskrcnn_benchmark/modeling/balanced_positive_negative_sampler.py b/maskrcnn_benchmark/modeling/balanced_positive_negative_sampler.py
# index ae6c323..c0bd004 100644
# --- a/maskrcnn_benchmark/modeling/balanced_positive_negative_sampler.py
# +++ b/maskrcnn_benchmark/modeling/balanced_positive_negative_sampler.py
# @@ -35,20 +35,22 @@ class BalancedPositiveNegativeSampler(object):
#          pos_idx = []
#          neg_idx = []
#          for matched_idxs_per_image in matched_idxs:
# -            device = matched_idxs_per_image.device
# -            matched_idxs_per_image = matched_idxs_per_image.cpu()
# -            assert matched_idxs_per_image.dim() == 1
# +            positive = torch.nonzero(matched_idxs_per_image >= 1).squeeze(1)
# +            negative = torch.nonzero(matched_idxs_per_image == 0).squeeze(1)
# +
#              num_pos = int(self.batch_size_per_image * self.positive_fraction)
# -            num_pos = min((matched_idxs_per_image > 0).sum(), num_pos)
# -            # TODO: investigate why torch.randperm with XLA device leads to loss nan.
# -            index_shuffle = torch.randperm(matched_idxs_per_image.size(0))
# -            matched_idxs_per_image_shuffle = matched_idxs_per_image[index_shuffle]
# -            labels_signed = torch.where(matched_idxs_per_image_shuffle <= 0,
# -                -matched_idxs_per_image_shuffle - 1, matched_idxs_per_image_shuffle)
# -            _, labels_shuffle_indices = torch.sort(labels_signed, descending=True)
# -            indices = index_shuffle[labels_shuffle_indices]
# -            pos_idx_per_image = indices[:num_pos]
# -            neg_idx_per_image = indices[-(self.batch_size_per_image - num_pos):]
# +            # protect against not enough positive examples
# +            num_pos = min(positive.numel(), num_pos)
# +            num_neg = self.batch_size_per_image - num_pos
# +            # protect against not enough negative examples
# +            num_neg = min(negative.numel(), num_neg)
# +
# +            # randomly select positive and negative examples
# +            perm1 = torch.randperm(positive.numel(), device=positive.device)[:num_pos]
# +            perm2 = torch.randperm(negative.numel(), device=negative.device)[:num_neg]
# +
# +            pos_idx_per_image = positive[perm1]
# +            neg_idx_per_image = negative[perm2]

#              # create binary mask from indices
#              pos_idx_per_image_mask = torch.zeros_like(
# @@ -60,11 +62,7 @@ class BalancedPositiveNegativeSampler(object):
#              pos_idx_per_image_mask[pos_idx_per_image] = 1
#              neg_idx_per_image_mask[neg_idx_per_image] = 1

# -            # protect against not enough positive and negative examples
# -            pos_idx_per_image_mask = pos_idx_per_image_mask & (matched_idxs_per_image > 0)
# -            neg_idx_per_image_mask = neg_idx_per_image_mask & (matched_idxs_per_image == 0)
# -
# -            pos_idx.append(pos_idx_per_image_mask.to(device=device))
# -            neg_idx.append(neg_idx_per_image_mask.to(device=device))
# +            pos_idx.append(pos_idx_per_image_mask)
# +            neg_idx.append(neg_idx_per_image_mask)

#          return pos_idx, neg_idx
# diff --git a/maskrcnn_benchmark/modeling/matcher.py b/maskrcnn_benchmark/modeling/matcher.py
# index d749e20..35ec5f1 100644
# --- a/maskrcnn_benchmark/modeling/matcher.py
# +++ b/maskrcnn_benchmark/modeling/matcher.py
# @@ -72,10 +72,8 @@ class Matcher(object):
#          between_thresholds = (matched_vals >= self.low_threshold) & (
#              matched_vals < self.high_threshold
#          )
# -        below_val = torch.full_like(matches, Matcher.BELOW_LOW_THRESHOLD)
# -        between_val = torch.full_like(matches, Matcher.BETWEEN_THRESHOLDS)
# -        matches = torch.where(below_low_threshold, below_val, matches)
# -        matches = torch.where(between_thresholds, between_val, matches)
# +        matches[below_low_threshold] = Matcher.BELOW_LOW_THRESHOLD
# +        matches[between_thresholds] = Matcher.BETWEEN_THRESHOLDS

#          if self.allow_low_quality_matches:
#              self.set_low_quality_matches_(matches, all_matches, match_quality_matrix)
# diff --git a/maskrcnn_benchmark/modeling/poolers.py b/maskrcnn_benchmark/modeling/poolers.py
# index 1365e52..9b3524d 100644
# --- a/maskrcnn_benchmark/modeling/poolers.py
# +++ b/maskrcnn_benchmark/modeling/poolers.py
# @@ -6,7 +6,6 @@ from torch import nn
#  from maskrcnn_benchmark.layers import ROIAlign

#  from .utils import cat
# -import torch_xla


#  class LevelMapper(object):
# @@ -115,14 +114,9 @@ class Pooler(nn.Module):
#              device=device,
#          )
#          for level, (per_level_feature, pooler) in enumerate(zip(x, self.poolers)):
# -            xla_device = per_level_feature.device
#              idx_in_level = torch.nonzero(levels == level).squeeze(1)
#              rois_per_level = rois[idx_in_level]
# -            torch_xla._XLAC._xla_sync_multi([per_level_feature, rois_per_level], devices=[])
# -            per_level_feature_cpu = per_level_feature.cpu().clone()
# -            rois_per_level_cpu = rois_per_level.cpu().clone()
# -
# -            result[idx_in_level] = pooler(per_level_feature_cpu, rois_per_level_cpu).to(xla_device)
# +            result[idx_in_level] = pooler(per_level_feature, rois_per_level)

#          return result

# diff --git a/maskrcnn_benchmark/modeling/roi_heads/box_head/inference.py b/maskrcnn_benchmark/modeling/roi_heads/box_head/inference.py
# index ea0e95f..595a2e6 100644
# --- a/maskrcnn_benchmark/modeling/roi_heads/box_head/inference.py
# +++ b/maskrcnn_benchmark/modeling/roi_heads/box_head/inference.py
# @@ -122,8 +122,6 @@ class PostProcessor(nn.Module):
#              boxes_j = boxes[inds, j * 4 : (j + 1) * 4]
#              boxlist_for_class = BoxList(boxes_j, boxlist.size, mode="xyxy")
#              boxlist_for_class.add_field("scores", scores_j)
# -            if len(boxlist_for_class) == 0:
# -                continue
#              boxlist_for_class = boxlist_nms(
#                  boxlist_for_class, self.nms
#              )
# @@ -135,8 +133,6 @@ class PostProcessor(nn.Module):

#          result = cat_boxlist(result)
#          number_of_detections = len(result)
# -        if number_of_detections == 0:
# -            return result

#          # Limit to max_per_image detections **over all classes**
#          if number_of_detections > self.detections_per_img > 0:
# diff --git a/maskrcnn_benchmark/modeling/roi_heads/box_head/loss.py b/maskrcnn_benchmark/modeling/roi_heads/box_head/loss.py
# index 6e56e6c..9f2771d 100644
# --- a/maskrcnn_benchmark/modeling/roi_heads/box_head/loss.py
# +++ b/maskrcnn_benchmark/modeling/roi_heads/box_head/loss.py
# @@ -2,7 +2,7 @@
#  import torch
#  from torch.nn import functional as F

# -from maskrcnn_benchmark.layers import smooth_l1_loss, smooth_l1_loss_no_reduction
# +from maskrcnn_benchmark.layers import smooth_l1_loss
#  from maskrcnn_benchmark.modeling.box_coder import BoxCoder
#  from maskrcnn_benchmark.modeling.matcher import Matcher
#  from maskrcnn_benchmark.structures.boxlist_ops import boxlist_iou
# @@ -63,13 +63,11 @@ class FastRCNNLossComputation(object):

#              # Label background (below the low threshold)
#              bg_inds = matched_idxs == Matcher.BELOW_LOW_THRESHOLD
# -            bg_val = torch.full_like(labels_per_image, 0)
# -            labels_per_image = torch.where(bg_inds, bg_val, labels_per_image)
# +            labels_per_image[bg_inds] = 0

#              # Label ignore proposals (between low and high thresholds)
#              ignore_inds = matched_idxs == Matcher.BETWEEN_THRESHOLDS
# -            ignore_val = torch.full_like(labels_per_image, -1)  # -1 is ignored by sampler
# -            labels_per_image = torch.where(ignore_inds, ignore_val, labels_per_image)
# +            labels_per_image[ignore_inds] = -1  # -1 is ignored by sampler

#              # compute regression targets
#              regression_targets_per_image = self.box_coder.encode(
# @@ -147,20 +145,23 @@ class FastRCNNLossComputation(object):

#          classification_loss = F.cross_entropy(class_logits, labels)

# -        assert labels.dim() == 1
# -        sampled_pos_inds_subset = torch.arange(labels.size(0))
# +        # get indices that correspond to the regression targets for
# +        # the corresponding ground truth labels, to be used with
# +        # advanced indexing
# +        sampled_pos_inds_subset = torch.nonzero(labels > 0).squeeze(1)
# +        labels_pos = labels[sampled_pos_inds_subset]
#          if self.cls_agnostic_bbox_reg:
#              map_inds = torch.tensor([4, 5, 6, 7], device=device)
#          else:
# -            map_inds = 4 * labels[:, None] + torch.tensor(
# +            map_inds = 4 * labels_pos[:, None] + torch.tensor(
#                  [0, 1, 2, 3], device=device)
# -        box_loss_elements = smooth_l1_loss_no_reduction(
# +
# +        box_loss = smooth_l1_loss(
#              box_regression[sampled_pos_inds_subset[:, None], map_inds],
#              regression_targets[sampled_pos_inds_subset],
#              size_average=False,
#              beta=1,
#          )
# -        box_loss = (box_loss_elements * (labels > 0).unsqueeze(1)).sum()
#          box_loss = box_loss / labels.numel()

#          return classification_loss, box_loss
# diff --git a/maskrcnn_benchmark/modeling/roi_heads/mask_head/loss.py b/maskrcnn_benchmark/modeling/roi_heads/mask_head/loss.py
# index e5e4287..36dcaa3 100644
# --- a/maskrcnn_benchmark/modeling/roi_heads/mask_head/loss.py
# +++ b/maskrcnn_benchmark/modeling/roi_heads/mask_head/loss.py
# @@ -63,33 +63,43 @@ class MaskRCNNLossComputation(object):
#          # NB: need to clamp the indices because we can have a single
#          # GT in the image, and matched_idxs can be -2, which goes
#          # out of bounds
# -        # TODO: rework this to not need CPU transfer
# -        matched_targets = target[matched_idxs.clamp(min=0).cpu()]
# +        matched_targets = target[matched_idxs.clamp(min=0)]
#          matched_targets.add_field("matched_idxs", matched_idxs)
#          return matched_targets

#      def prepare_targets(self, proposals, targets):
#          labels = []
#          masks = []
# -        matched_idxs = []
#          for proposals_per_image, targets_per_image in zip(proposals, targets):
#              matched_targets = self.match_targets_to_proposals(
#                  proposals_per_image, targets_per_image
#              )
# +            matched_idxs = matched_targets.get_field("matched_idxs")

#              labels_per_image = matched_targets.get_field("labels")
#              labels_per_image = labels_per_image.to(dtype=torch.int64)

# +            # this can probably be removed, but is left here for clarity
# +            # and completeness
# +            neg_inds = matched_idxs == Matcher.BELOW_LOW_THRESHOLD
# +            labels_per_image[neg_inds] = 0
# +
# +            # mask scores are only computed on positive samples
# +            positive_inds = torch.nonzero(labels_per_image > 0).squeeze(1)
# +
#              segmentation_masks = matched_targets.get_field("masks")
# +            segmentation_masks = segmentation_masks[positive_inds]
# +
# +            positive_proposals = proposals_per_image[positive_inds]
# +
#              masks_per_image = project_masks_on_boxes(
# -                segmentation_masks, proposals_per_image, self.discretization_size
# +                segmentation_masks, positive_proposals, self.discretization_size
#              )

#              labels.append(labels_per_image)
#              masks.append(masks_per_image)
# -            matched_idxs.append(matched_targets.get_field("matched_idxs"))

# -        return labels, masks, matched_idxs
# +        return labels, masks

#      def __call__(self, proposals, mask_logits, targets):
#          """
# @@ -101,25 +111,23 @@ class MaskRCNNLossComputation(object):
#          Return:
#              mask_loss (Tensor): scalar tensor containing the loss
#          """
# -        labels, mask_targets, matched_idxs = self.prepare_targets(proposals, targets)
# +        labels, mask_targets = self.prepare_targets(proposals, targets)

#          labels = cat(labels, dim=0)
#          mask_targets = cat(mask_targets, dim=0)
# -        matched_idxs = cat(matched_idxs, dim=0)
# +
# +        positive_inds = torch.nonzero(labels > 0).squeeze(1)
# +        labels_pos = labels[positive_inds]

#          # torch.mean (in binary_cross_entropy_with_logits) doesn't
#          # accept empty tensors, so handle it separately
#          if mask_targets.numel() == 0:
#              return mask_logits.sum() * 0

# -        assert labels.dim() == 1
# -        inds = torch.arange(labels.size(0))
#          mask_loss = F.binary_cross_entropy_with_logits(
# -            mask_logits[inds, labels], mask_targets, reduction='none'
# +            mask_logits[positive_inds, labels_pos], mask_targets
#          )
# -        mask_loss_fact = (matched_idxs != Matcher.BELOW_LOW_THRESHOLD).to(dtype=torch.float32).unsqueeze(1).unsqueeze(1)
# -        mask_loss_fact = mask_loss_fact.expand_as(mask_loss)
# -        return (mask_loss * mask_loss_fact).sum() / mask_loss_fact.sum()
# +        return mask_loss


#  def make_roi_mask_loss_evaluator(cfg):
# diff --git a/maskrcnn_benchmark/modeling/roi_heads/mask_head/mask_head.py b/maskrcnn_benchmark/modeling/roi_heads/mask_head/mask_head.py
# index ea61680..a9ce245 100644
# --- a/maskrcnn_benchmark/modeling/roi_heads/mask_head/mask_head.py
# +++ b/maskrcnn_benchmark/modeling/roi_heads/mask_head/mask_head.py
# @@ -62,8 +62,10 @@ class ROIMaskHead(torch.nn.Module):
#          if self.training:
#              # during training, only focus on positive boxes
#              all_proposals = proposals
# +            proposals, positive_inds = keep_only_positive_boxes(proposals)
#          if self.training and self.cfg.MODEL.ROI_MASK_HEAD.SHARE_BOX_FEATURE_EXTRACTOR:
#              x = features
# +            x = x[torch.cat(positive_inds, dim=0)]
#          else:
#              x = self.feature_extractor(features, proposals)
#          mask_logits = self.predictor(x)
# diff --git a/maskrcnn_benchmark/modeling/rpn/loss.py b/maskrcnn_benchmark/modeling/rpn/loss.py
# index 086167f..840e354 100644
# --- a/maskrcnn_benchmark/modeling/rpn/loss.py
# +++ b/maskrcnn_benchmark/modeling/rpn/loss.py
# @@ -125,10 +125,8 @@ class RPNLossComputation(object):
#          ) / (sampled_inds.numel())

#          objectness_loss = F.binary_cross_entropy_with_logits(
# -            objectness, labels, reduction='none'
# +            objectness[sampled_inds], labels[sampled_inds]
#          )
# -        objectness_loss = torch.where(sampled_inds, objectness_loss, torch.zeros_like(objectness_loss))
# -        objectness_loss = objectness_loss.sum() / sampled_inds.sum()

#          return objectness_loss, box_loss

# diff --git a/maskrcnn_benchmark/structures/boxlist_ops.py b/maskrcnn_benchmark/structures/boxlist_ops.py
# index f22cf5d..dc51212 100644
# --- a/maskrcnn_benchmark/structures/boxlist_ops.py
# +++ b/maskrcnn_benchmark/structures/boxlist_ops.py
# @@ -5,7 +5,6 @@ from .bounding_box import BoxList

#  from maskrcnn_benchmark.layers import nms as _box_nms

# -import torch_xla

#  def boxlist_nms(boxlist, nms_thresh, max_proposals=-1, score_field="scores"):
#      """
# @@ -25,16 +24,9 @@ def boxlist_nms(boxlist, nms_thresh, max_proposals=-1, score_field="scores"):
#      boxlist = boxlist.convert("xyxy")
#      boxes = boxlist.bbox
#      score = boxlist.get_field(score_field)
# -    device = boxes.device
# -    torch_xla._XLAC._xla_sync_multi([boxes, score], devices=[])
# -    boxes_cpu = boxes.cpu().clone()
# -    score_cpu = score.cpu().clone()
# -    keep = _box_nms(boxes_cpu, score_cpu, nms_thresh)
# -
# -    # keep = _box_nms(boxes, score, nms_thresh)
# +    keep = _box_nms(boxes, score, nms_thresh)
#      if max_proposals > 0:
#          keep = keep[: max_proposals]
# -    keep = keep.to(device=device)
#      boxlist = boxlist[keep]
#      return boxlist.convert(mode)

# @@ -84,8 +76,8 @@ def boxlist_iou(boxlist1, boxlist2):

#      box1, box2 = boxlist1.bbox, boxlist2.bbox

# -    lt = torch.max(box1[:, None, :2], box2[:, :2]) # [N,M,2]
# -    rb = torch.min(box1[:, None, 2:], box2[:, 2:]) # [N,M,2]
# +    lt = torch.max(box1[:, None, :2], box2[:, :2])  # [N,M,2]
# +    rb = torch.min(box1[:, None, 2:], box2[:, 2:])  # [N,M,2]

#      TO_REMOVE = 1

# diff --git a/maskrcnn_benchmark/utils/model_zoo.py b/maskrcnn_benchmark/utils/model_zoo.py
# index d9e4b59..7a0ebb3 100644
# --- a/maskrcnn_benchmark/utils/model_zoo.py
# +++ b/maskrcnn_benchmark/utils/model_zoo.py
# @@ -2,7 +2,9 @@
#  import os
#  import sys

# -from torch.hub import _download_url_to_file, urlparse, HASH_REGEX
# +from torch.utils.model_zoo import _download_url_to_file
# +from torch.utils.model_zoo import urlparse
# +from torch.utils.model_zoo import HASH_REGEX

#  from maskrcnn_benchmark.utils.comm import is_main_process
#  from maskrcnn_benchmark.utils.comm import synchronize
# diff --git a/tests/test_roi_align.py b/tests/test_roi_align.py
# deleted file mode 100644
# index f549d8a..0000000
# --- a/tests/test_roi_align.py
# +++ /dev/null
# @@ -1,54 +0,0 @@
# -from maskrcnn_benchmark.layers import ROIAlign, tensor_roi_align
# -import random
# -import torch
# -import torch_xla
# -import torch_xla_py.xla_model as xm
# -import unittest
# -
# -
# -NUM_ROIS = 512
# -
# -class TestROIAlign(unittest.TestCase):
# -
# -  def _gen_rois(self, batch_size, width, height, rois_count):
# -    rois = []
# -    for i in range(0, rois_count):
# -      batch_id = float(int(i / NUM_ROIS))
# -      start_w = 0.5 * random.random() * width
# -      start_h = 0.5 * random.random() * height
# -      end_w = (0.5 + 0.5 * random.random()) * width
# -      end_h = (0.5 + 0.5 * random.random()) * height
# -      rois.append(torch.tensor([batch_id, start_w, start_h, end_w, end_h]))
# -    return torch.stack(rois, dim=0)
# -
# -  def _gen_data(self, batch_size, channels, width, height):
# -    return torch.rand([batch_size, channels, width, height])
# -
# -  def test_roi_align(self):
# -    batch_size = 1
# -    rois_count = NUM_ROIS * batch_size
# -    width = 64
# -    height = 64
# -    channels = 1024
# -    output_size = (14, 14)
# -    spatial_scale = .5
# -    sampling_ratio = 2
# -    rois = self._gen_rois(batch_size, width, height, rois_count)
# -    rois_3d = rois[:, 1:].reshape(batch_size, NUM_ROIS, -1)
# -    bottom_image_data = self._gen_data(batch_size, channels, width, height)
# -    roi_align = ROIAlign(
# -        output_size, spatial_scale=spatial_scale, sampling_ratio=sampling_ratio)
# -    aligned_ref = roi_align(bottom_image_data, rois)
# -    xla_device = xm.xla_device()
# -    rois_3d = rois_3d.to(device=xla_device)
# -    bottom_image_data = bottom_image_data.to(device=xla_device)
# -    aligned = tensor_roi_align(bottom_image_data, rois_3d, output_size,
# -                               spatial_scale, sampling_ratio)
# -    self.assertAlmostEqual(
# -        (aligned - aligned_ref).abs().max().item(), 0, places=5)
# -
# -
# -if __name__ == '__main__':
# -  torch.manual_seed(42)
# -  random.seed(42)
# -  unittest.main()
# diff --git a/tools/train_net.py b/tools/train_net.py
# index 2481bf8..e4f95f0 100644
# --- a/tools/train_net.py
# +++ b/tools/train_net.py
# @@ -6,6 +6,7 @@ Basic training script for PyTorch
#  # Set up custom environment before nearly anything else is imported
#  # NOTE: this should be the first import (no not reorder)
#  from maskrcnn_benchmark.utils.env import setup_environment  # noqa F401 isort:skip
# +
#  import argparse
#  import os

# @@ -24,14 +25,10 @@ from maskrcnn_benchmark.utils.imports import import_file
#  from maskrcnn_benchmark.utils.logger import setup_logger
#  from maskrcnn_benchmark.utils.miscellaneous import mkdir

# -import torch_xla
# -import torch_xla_py.utils as xu
# -import torch_xla_py.xla_model as xm

#  def train(cfg, local_rank, distributed):
#      model = build_detection_model(cfg)
# -    device = xm.xla_device()
# -    torch_xla._XLAC._xla_set_default_device(str(device))
# +    device = torch.device(cfg.MODEL.DEVICE)
#      model.to(device)

#      optimizer = make_optimizer(cfg, model)
# @@ -80,9 +77,9 @@ def train(cfg, local_rank, distributed):


#  def run_test(cfg, model, distributed):
# -    device = xm.xla_device()
#      if distributed:
#          model = model.module
# +    torch.cuda.empty_cache()  # TODO check if it helps
#      iou_types = ("bbox",)
#      if cfg.MODEL.MASK_ON:
#          iou_types = iou_types + ("segm",)
# @@ -103,12 +100,12 @@ def run_test(cfg, model, distributed):
#              dataset_name=dataset_name,
#              iou_types=iou_types,
#              box_only=False if cfg.MODEL.RETINANET_ON else cfg.MODEL.RPN_ONLY,
# -            device=device,
# +            device=cfg.MODEL.DEVICE,
#              expected_results=cfg.TEST.EXPECTED_RESULTS,
#              expected_results_sigma_tol=cfg.TEST.EXPECTED_RESULTS_SIGMA_TOL,
#              output_folder=output_folder,
#          )
# -        # synchronize()
# +        synchronize()


#  def main():
# @@ -137,7 +134,7 @@ def main():
#      args = parser.parse_args()

#      num_gpus = int(os.environ["WORLD_SIZE"]) if "WORLD_SIZE" in os.environ else 1
# -    args.distributed = False
# +    args.distributed = num_gpus > 1

#      if args.distributed:
#          torch.cuda.set_device(args.local_rank)
# @@ -167,7 +164,7 @@ def main():
#          logger.info(config_str)
#      logger.info("Running with config:\n{}".format(cfg))

# -    model = train(cfg, args.local_rank, False)
# +    model = train(cfg, args.local_rank, args.distributed)

#      if not args.skip_test:
#          run_test(cfg, model, args.distributed)
